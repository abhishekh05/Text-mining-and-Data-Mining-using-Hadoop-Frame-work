{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import cursor\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nytimesarticle import articleAPI\n",
    "import tweepy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"foHnIoTVSVucGhY55JACCWVe3\"\n",
    "consumer_secret = \"F9ny66tppNfLlKEi5G3yaEuuc99FE3rWOyhj4tvtgFVnWmM46y\"\n",
    "access_token = \"995973489747259395-QWSc6wcToKypIfhGZz5z1IiGVW7FKee\"\n",
    "access_token_secret = \"UWJVzpl4pcO6wleoQQwCVHwoHWp5y501RUF5w8QMQifk0\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    query_1= \"Immigrants\"\n",
    "    query_2= \"Immigration\"\n",
    "    query_3 = \"#immigration\"\n",
    "    query_4 = \"#immigrationpolicy\"\n",
    "    query_5 = \"#legalimmigration\"\n",
    "    query_6 = \"#illegalimmigration\"\n",
    "    query_7 = \"#immigrationlaws\"\n",
    "    query_8 = \"#BorderWall\"\n",
    "    query_9 = \"#daca\"\n",
    "    query_10 =\"#immigrationcrisis\"\n",
    "    query_11 = \"#trump\"\n",
    "    query_12 =  \"#potus\"\n",
    "#    results_1 = api.search(query_1,count= 1000000, lang=\"en\")\n",
    "#     results_2 = api.search(query_2,count= 1000000, lang=\"en\")\n",
    "#     results_3 = api.search(query_3,count= 1000000, lang=\"en\")\n",
    "#     results_4 = api.search(query_4,count= 1000000, lang=\"en\")\n",
    "#     results_5 = api.search(query_5,count= 1000000, lang=\"en\")\n",
    "#     results_6 = api.search(query_6,count= 1000000, lang=\"en\")\n",
    "#     results_7 = api.search(query_7,count= 1000000, lang=\"en\")\n",
    "#     results_8= api.search(query_8,count= 1000000, lang=\"en\")\n",
    "#     results_9 = api.search( query_9,count =1000000, lang=\"en\")\n",
    "#     results_10 = api.search( query_10,count =1000000, lang=\"en\")\n",
    "#     results_11 = api.search( query_11,count =1000000, lang=\"en\")\n",
    "#     results_12 = api.search( query_12,count =1000000, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"tweet/immigrants.txt\",\"a\",encoding=\"utf-8\") \n",
    "file2 = open(\"tweet/immigration.txt\",\"a\",encoding=\"utf-8\") \n",
    "file3 = open(\"tweet/immigrationpolicy.txt\",\"a\",encoding=\"utf-8\") \n",
    "file4 = open(\"tweet/legalimmigration.txt\",\"a\",encoding=\"utf-8\") \n",
    "file5 = open(\"tweet/illegalimmigration.txt\",\"a\",encoding=\"utf-8\")\n",
    "file6 = open(\"tweet/immigrationlaws.txt\",\"a\",encoding=\"utf-8\") \n",
    "file7 = open(\"tweet/BorderWall.txt\",\"a\",encoding=\"utf-8\") \n",
    "file8 = open(\"tweet/daca.txt\",\"a\",encoding=\"utf-8\") \n",
    "file9 = open(\"tweet/immigrationcrisis.txt\",\"a\",encoding=\"utf-8\") \n",
    "file10 = open(\"tweet/trump.txt\",\"a\",encoding=\"utf-8\") \n",
    "file11 = open(\"tweet/potus.txt\",\"a\",encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    loop = 0\n",
    "    last_1 = None\n",
    "    last_2 = None\n",
    "    last_3 = None\n",
    "    last_4 = None\n",
    "    last_5 = None\n",
    "    last_6 = None\n",
    "    last_7 = None\n",
    "    last_8 = None\n",
    "    last_9 = None\n",
    "    last_10 = None\n",
    "    last_11 = None\n",
    "    last_12 = None\n",
    "    print(\"working\")\n",
    "    while True:\n",
    "        results_1 = api.search(query_1,count= 10000, max_id=last_1,since=\"2019-01-01\",lang=\"en\")\n",
    "        results_2 = api.search(query_2,count= 1000000, max_id=last_2,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_3 = api.search(query_3,count= 1000000, max_id=last_3,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_4 = api.search(query_4,count= 1000000, max_id=last_4,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_5 = api.search(query_5,count= 1000000, max_id=last_5,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_6 = api.search(query_6,count= 1000000, max_id=last_6,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_7 = api.search(query_7,count= 1000000, max_id=last_7,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_8= api.search(query_8,count= 1000000, max_id=last_8,since=\"2019-01-01\" ,lang=\"en\")\n",
    "        results_9 = api.search( query_9,count =1000000, max_id=last_9,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_10 = api.search( query_10,count =1000000, max_id=last_10,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_11 = api.search( query_11,count =1000000, max_id=last_11,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_12 = api.search( query_12,count =1000000, max_id=last_12,since=\"2019-01-01\", lang=\"en\")\n",
    "        \n",
    "        \n",
    "        for tweet in results_1:\n",
    "        #print(tweet.user.screen_name,\"Tweeted:\",tweet.text)\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "               # print(tweet.text)\n",
    "               file1.write(tweet.text)\n",
    "                \n",
    "        for tweet in results_2:\n",
    "        #     print(tweet.user.screen_name,\"Tweeted:\",tweet.text) \n",
    "             if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file2.write(tweet.text)\n",
    "\n",
    "        for tweet in results_3:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file2.write(tweet.text)\n",
    "\n",
    "        for tweet in results_4:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file3.write(tweet.text)\n",
    "        for tweet in results_5:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file4.write(tweet.text)\n",
    "        print(\"its still running\")\n",
    "        for tweet in results_6:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file5.write(tweet.text)\n",
    "        for tweet in results_7:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file6.write(tweet.text)\n",
    "        for tweet in results_8:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file7.write(tweet.text)\n",
    "        for tweet in results_9:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file8.write(tweet.text)\n",
    "        for tweet in results_10:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file9.write(tweet.text)\n",
    "        for tweet in results_11:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file10.write(tweet.text)\n",
    "        for tweet in results_12:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file11.write(tweet.text)\n",
    "\n",
    "        #print( \"results in the first iter\",results_4)\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        print(\"length of result_1 :\",len(results_1))\n",
    "        if  len(results_1)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_1 = results_1[-1]._json['id'] - 1\n",
    "        if  len(results_2)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_2 = results_2[-1]._json['id'] - 1\n",
    "        if  len(results_3)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_3 = results_3[-1]._json['id'] - 1\n",
    "        if  len(results_4)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_4 = results_4[-1]._json['id'] - 1\n",
    "        if  len(results_5)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_5 = results_5[-1]._json['id'] - 1\n",
    "        if  len(results_6)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_6 = results_6[-1]._json['id'] - 1\n",
    "        if  len(results_7)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_7 = results_7[-1]._json['id'] - 1\n",
    "        if  len(results_8)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_8 = results_8[-1]._json['id'] - 1\n",
    "        if  len(results_9)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_9 = results_9[-1]._json['id'] - 1\n",
    "        if  len(results_10)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_10 =results_10[-1]._json['id'] - 1\n",
    "        if  len(results_11)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_11 = results_11[-1]._json['id'] - 1\n",
    "        if  len(results_2)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_12 = results_12[-1]._json['id'] - 1\n",
    "        loop+=1\n",
    "        print(loop)\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "Thank god its still running\n",
      "length of result_1 : 85\n",
      "1\n",
      "Thank god its still running\n",
      "length of result_1 : 79\n",
      "empty\n",
      "empty\n",
      "2\n",
      "Thank god its still running\n",
      "length of result_1 : 98\n",
      "empty\n",
      "empty\n",
      "3\n",
      "Thank god its still running\n",
      "length of result_1 : 84\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "4\n",
      "Thank god its still running\n",
      "length of result_1 : 90\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "5\n",
      "Thank god its still running\n",
      "length of result_1 : 90\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "6\n",
      "Thank god its still running\n",
      "length of result_1 : 92\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "7\n",
      "Thank god its still running\n",
      "length of result_1 : 81\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "8\n",
      "Thank god its still running\n",
      "length of result_1 : 89\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "9\n",
      "Thank god its still running\n",
      "length of result_1 : 82\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "10\n",
      "Thank god its still running\n",
      "length of result_1 : 84\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "11\n",
      "Thank god its still running\n",
      "length of result_1 : 88\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "12\n",
      "Thank god its still running\n",
      "length of result_1 : 87\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "13\n",
      "Thank god its still running\n",
      "length of result_1 : 87\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "14\n",
      "Thank god its still running\n",
      "length of result_1 : 96\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "15\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "[{'message': 'Rate limit exceeded', 'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5e10863a7773>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperiodic_work\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-dc6bfb1274bb>\u001b[0m in \u001b[0;36mperiodic_work\u001b[1;34m(interval)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"working\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mresults_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlast_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2019-01-01\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mresults_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlast_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2019-01-01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mresults_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlast_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2019-01-01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_rate_limit_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: [{'message': 'Rate limit exceeded', 'code': 88}]"
     ]
    }
   ],
   "source": [
    "periodic_work(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()\n",
    "file4.close()\n",
    "file5.close()\n",
    "file5.close()\n",
    "file6.close()\n",
    "file7.close()\n",
    "file8.close()\n",
    "file9.close()\n",
    "file10.close()\n",
    "file11.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Articles using New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api = articleAPI('9seo8sPL5rpxJkh1d49M32Vt07FACoIc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    for i in articles['response']['docs']:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "#         if i['abstract'] is not None:\n",
    "#             dic['abstract'] = i['abstract'].encode(\"utf8\")\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        #dic['desk'] = i['news_desk']\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        dic['section'] = i['section_name']\n",
    "        if i['snippet'] is not None:\n",
    "            dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "        #dic['source'] = i['source']\n",
    "     #   dic['type'] = i['type_of_material']\n",
    "        dic['url'] = i['web_url']\n",
    "        dic['word_count'] = i['word_count']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'glocations' in i['keywords'][x]['name']:\n",
    "                locations.append(i['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'subject' in i['keywords'][x]['name']:\n",
    "                subjects.append(i['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects   \n",
    "        news.append(dic)\n",
    "    return(news) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1= \"Immigrants\"\n",
    "query_2= \"Immigration\"\n",
    "query_3 =\"visa\"\n",
    "query_4 = \"policy\"\n",
    "query_5 = \"legal immigration\"\n",
    "query_6 = \"illegal immigration\"\n",
    "query_7 = \"immigration laws\"\n",
    "query_8 = \"Mexican Immigrants\"\n",
    "query_9 = \"daca\"\n",
    "query_10 = \"immigration crisis\"\n",
    "articles_1= api.search(q = query_1,begin_date = 20190328,end_date =20190404)\n",
    "articles_2= api.search(q = query_2,begin_date = 20190328,end_date =20190404)\n",
    "articles_3= api.search(q = query_3,begin_date = 20190328,end_date =20190404)\n",
    "articles_4= api.search(q = query_4,begin_date = 20190328,end_date =20190404)\n",
    "articles_5= api.search(q = query_5,begin_date = 20190328,end_date =20190404)\n",
    "articles_6= api.search(q = query_6,begin_date = 20190328,end_date =20190404)\n",
    "articles_7= api.search(q = query_7,begin_date = 20190328,end_date =20190404)\n",
    "articles_8 = api.search(q= query_8,begin_date = 20190328,end_date =20190404) \n",
    "articles_9 = api.search(q= query_9,begin_date = 20190328,end_date =20190404)\n",
    "articles_10=api.search(q=query_10,begin_date = 20190328,end_date =20190404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_1 = parse_articles(articles_1)\n",
    "parsed_2 = parse_articles(articles_2)\n",
    "parsed_3 = parse_articles(articles_3)\n",
    "parsed_4 = parse_articles(articles_4)\n",
    "parsed_5 = parse_articles(articles_5)\n",
    "parsed_6 = parse_articles(articles_6)\n",
    "parsed_7 = parse_articles(articles_7)\n",
    "parsed_8 =  parse_articles(articles_8)\n",
    "parsed_9 =  parse_articles(articles_9 )\n",
    "parsed_10 =  parse_articles(articles_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different keywords we have made different files and made sure we do not have any duplicate articles in our text file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigrants Key word and Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "immigrants_keyword=[]\n",
    "key1 = open(\"data/immigrants.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_1)):\n",
    "    page_link = parsed_1[i]['url']\n",
    "    immigrants_keyword.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            #print(paragraphs)\n",
    "            key1.write(paragraphs)\n",
    "with open('data/immigrants_nytimes.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([immigrants_keyword])\n",
    "key1.close()\n",
    "csvFile.close()\n",
    "print(immigrants_keyword)\n",
    "len(immigrants_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Key word and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Immigration_keyword=[]\n",
    "key2 = open(\"data/immigrantion1.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_2)):\n",
    "    page_link_i = parsed_2[i]['url']\n",
    "    Immigration_keyword.append(page_link_i)\n",
    "    page_response_i = requests.get(page_link_i, timeout=5)\n",
    "    soup = BeautifulSoup(page_response_i.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key2.write(paragraphs)\n",
    "with open('data/immigration_nytimes.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([Immigration_keyword])\n",
    "key2.close()\n",
    "csvFile.close()\n",
    "print(Immigration_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Immigration_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visa Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visa_keyword=[]\n",
    "key3 = open(\"data/visa.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_3)):\n",
    "    page_link_v = parsed_3[i]['url']\n",
    "    Visa_keyword.append(page_link_v)\n",
    "    page_response_v = requests.get(page_link_v, timeout=5)\n",
    "    soup_v = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup_v.find_all(\"p\"))):\n",
    "            paragraphs = soup_v.find_all(\"p\")[tag].text\n",
    "            #print(\"paragraphs\",paragraphs)\n",
    "            key3.write(paragraphs)\n",
    "            \n",
    "with open('data/visa.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([Visa_keyword])\n",
    "key3.close()\n",
    "csvFile.close()\n",
    "print(Visa_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Visa_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_immigration_keyword=[]\n",
    "key4 = open(\"data/policy.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_4)):\n",
    "    page_link = parsed_4[i]['url']\n",
    "    policy_immigration_keyword.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key4.write(paragraphs)\n",
    "with open('data/policy.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([policy_immigration_keyword])\n",
    "key4.close()\n",
    "csvFile.close()\n",
    "print(policy_immigration_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(policy_immigration_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_immigration_keyword=[]\n",
    "key5 = open(\"data/legalweek1.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_5)):\n",
    "    page_link = parsed_5[i]['url']\n",
    "    legal_immigration_keyword.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key5.write(paragraphs)\n",
    "with open('data/legalweek1.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([legal_immigration_keyword])\n",
    "key5.close()\n",
    "csvFile.close()\n",
    "print(legal_immigration_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illegal Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_immigration=[]\n",
    "key6 = open(\"data/illegalweek1.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_6)):\n",
    "    page_link = parsed_6[i]['url']\n",
    "    illegal_immigration.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key6.write(paragraphs)\n",
    "with open('data/illegalweek1.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([illegal_immigration])\n",
    "key6.close()\n",
    "csvFile.close()\n",
    "print(illegal_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(illegal_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_laws=[]\n",
    "key7 = open(\"data/lawsweek1.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_7)):\n",
    "    page_link = parsed_7[i]['url']\n",
    "    immigration_laws.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key7.write(paragraphs)\n",
    "with open('data/lawsweek1.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([immigration_laws])\n",
    "key7.close()\n",
    "csvFile.close()\n",
    "print(immigration_laws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(immigration_laws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexican Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexican_immigration=[]\n",
    "key8 = open(\"data/mexican_immigrantsweek1.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_7)):\n",
    "    page_link = parsed_7[i]['url']\n",
    "    mexican_immigration.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            key8.write(paragraphs)\n",
    "with open('data/mexican_immigrantweek1.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([mexican_immigration])\n",
    "key7.close()\n",
    "csvFile.close()\n",
    "print(mexican_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mexican_immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daca_keyword=[]\n",
    "key9 = open(\"data/daca.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_9)):\n",
    "    page_link = parsed_9[i]['url']\n",
    "    daca_keyword.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            #print(paragraphs)\n",
    "            key9.write(paragraphs)\n",
    "with open('data/daca_nytimes.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([daca_keyword])\n",
    "key9.close()\n",
    "csvFile.close()\n",
    "print(daca_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_keyword=[]\n",
    "key10 = open(\"data/crisis.txt\",\"a\",encoding=\"utf-8\") \n",
    "for i in range(0,len(parsed_10)):\n",
    "    page_link = parsed_10[i]['url']\n",
    "    crisis_keyword.append(page_link)\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")        \n",
    "\n",
    "    for  tag in range(0, len(soup.find_all(\"p\"))):\n",
    "            paragraphs = soup.find_all(\"p\")[tag].text\n",
    "            #print(paragraphs)\n",
    "            key10.write(paragraphs)\n",
    "with open('data/crisis_nytimes.csv', 'a') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows([crisis_keyword])\n",
    "key10.close()\n",
    "csvFile.close()\n",
    "print(crisis_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"data/crisis_nytimes.csv\")\n",
    "b = pd.read_csv(\"data/daca_nytimes.csv\")\n",
    "c = pd.read_csv(\"data/illegalweek1.csv\")\n",
    "d = pd.read_csv(\"data/immigrants_nytimes.csv\")\n",
    "e = pd.read_csv(\"data/immigration_nytimes.csv\")\n",
    "f = pd.read_csv(\"data/lawsweek1.csv\")\n",
    "g = pd.read_csv(\"data/legalweek1.csv\")\n",
    "h = pd.read_csv(\"data/mexican_immigrantweek1.csv\")\n",
    "i = pd.read_csv(\"data/policy.csv\")\n",
    "j = pd.read_csv(\"data/visa.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
