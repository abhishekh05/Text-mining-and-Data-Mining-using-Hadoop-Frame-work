{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import cursor\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nytimesarticle import articleAPI\n",
    "import tweepy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"foHnIoTVSVucGhY55JACCWVe3\"\n",
    "consumer_secret = \"F9ny66tppNfLlKEi5G3yaEuuc99FE3rWOyhj4tvtgFVnWmM46y\"\n",
    "access_token = \"995973489747259395-QWSc6wcToKypIfhGZz5z1IiGVW7FKee\"\n",
    "access_token_secret = \"UWJVzpl4pcO6wleoQQwCVHwoHWp5y501RUF5w8QMQifk0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    query_1= \"Immigrants\"\n",
    "    query_2= \"Immigration\"\n",
    "    query_3 = \"#immigration\"\n",
    "    query_4 = \"#immigrationpolicy\"\n",
    "    query_5 = \"#legalimmigration\"\n",
    "    query_6 = \"#illegalimmigration\"\n",
    "    query_7 = \"#immigrationlaws\"\n",
    "    query_8 = \"#BorderWall\"\n",
    "    query_9 = \"#daca\"\n",
    "    query_10 =\"#immigrationcrisis\"\n",
    "    query_11 = \"#trump\"\n",
    "    query_12 =  \"#potus\"\n",
    "#    results_1 = api.search(query_1,count= 1000000, lang=\"en\")\n",
    "#     results_2 = api.search(query_2,count= 1000000, lang=\"en\")\n",
    "#     results_3 = api.search(query_3,count= 1000000, lang=\"en\")\n",
    "#     results_4 = api.search(query_4,count= 1000000, lang=\"en\")\n",
    "#     results_5 = api.search(query_5,count= 1000000, lang=\"en\")\n",
    "#     results_6 = api.search(query_6,count= 1000000, lang=\"en\")\n",
    "#     results_7 = api.search(query_7,count= 1000000, lang=\"en\")\n",
    "#     results_8= api.search(query_8,count= 1000000, lang=\"en\")\n",
    "#     results_9 = api.search( query_9,count =1000000, lang=\"en\")\n",
    "#     results_10 = api.search( query_10,count =1000000, lang=\"en\")\n",
    "#     results_11 = api.search( query_11,count =1000000, lang=\"en\")\n",
    "#     results_12 = api.search( query_12,count =1000000, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/immigrants.txt\",\"a\",encoding=\"utf-8\") \n",
    "file2 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/immigration.txt\",\"a\",encoding=\"utf-8\") \n",
    "file3 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/immigrationpolicy.txt\",\"a\",encoding=\"utf-8\") \n",
    "file4 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/legalimmigration.txt\",\"a\",encoding=\"utf-8\") \n",
    "file5 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/illegalimmigration.txt\",\"a\",encoding=\"utf-8\")\n",
    "file6 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/immigrationlaws.txt\",\"a\",encoding=\"utf-8\") \n",
    "file7 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/BorderWall.txt\",\"a\",encoding=\"utf-8\") \n",
    "file8 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/daca.txt\",\"a\",encoding=\"utf-8\") \n",
    "file9 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/immigrationcrisis.txt\",\"a\",encoding=\"utf-8\") \n",
    "file10 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/trump.txt\",\"a\",encoding=\"utf-8\") \n",
    "file11 = open(\"D:/Data Intensive Computing/Lab2/Ash data/DATA/twitter/potus.txt\",\"a\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    loop = 0\n",
    "    last_1 = None\n",
    "    last_2 = None\n",
    "    last_3 = None\n",
    "    last_4 = None\n",
    "    last_5 = None\n",
    "    last_6 = None\n",
    "    last_7 = None\n",
    "    last_8 = None\n",
    "    last_9 = None\n",
    "    last_10 = None\n",
    "    last_11 = None\n",
    "    last_12 = None\n",
    "    print(\"working\")\n",
    "    while True:\n",
    "        results_1 = api.search(query_1,count= 10000, max_id=last_1,since=\"2019-01-01\",lang=\"en\")\n",
    "        results_2 = api.search(query_2,count= 1000000, max_id=last_2,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_3 = api.search(query_3,count= 1000000, max_id=last_3,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_4 = api.search(query_4,count= 1000000, max_id=last_4,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_5 = api.search(query_5,count= 1000000, max_id=last_5,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_6 = api.search(query_6,count= 1000000, max_id=last_6,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_7 = api.search(query_7,count= 1000000, max_id=last_7,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_8= api.search(query_8,count= 1000000, max_id=last_8,since=\"2019-01-01\" ,lang=\"en\")\n",
    "        results_9 = api.search( query_9,count =1000000, max_id=last_9,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_10 = api.search( query_10,count =1000000, max_id=last_10,since=\"2019-01-01\" , lang=\"en\")\n",
    "        results_11 = api.search( query_11,count =1000000, max_id=last_11,since=\"2019-01-01\", lang=\"en\")\n",
    "        results_12 = api.search( query_12,count =1000000, max_id=last_12,since=\"2019-01-01\", lang=\"en\")\n",
    "        \n",
    "        \n",
    "        for tweet in results_1:\n",
    "        #print(tweet.user.screen_name,\"Tweeted:\",tweet.text)\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "               # print(tweet.text)\n",
    "               file1.write(tweet.text)\n",
    "                \n",
    "        for tweet in results_2:\n",
    "        #     print(tweet.user.screen_name,\"Tweeted:\",tweet.text) \n",
    "             if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file2.write(tweet.text)\n",
    "\n",
    "        for tweet in results_3:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file2.write(tweet.text)\n",
    "\n",
    "        for tweet in results_4:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file3.write(tweet.text)\n",
    "        for tweet in results_5:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file4.write(tweet.text)\n",
    "        print(\"its still running\")\n",
    "        for tweet in results_6:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file5.write(tweet.text)\n",
    "        for tweet in results_7:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file6.write(tweet.text)\n",
    "        for tweet in results_8:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    #print(tweet.text)\n",
    "                    file7.write(tweet.text)\n",
    "        for tweet in results_9:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                   # print(tweet.text)\n",
    "                    file8.write(tweet.text)\n",
    "        for tweet in results_10:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file9.write(tweet.text)\n",
    "        for tweet in results_11:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file10.write(tweet.text)\n",
    "        for tweet in results_12:\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                    enumerate(tweet.text)\n",
    "                    file11.write(tweet.text)\n",
    "\n",
    "        #print( \"results in the first iter\",results_4)\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        print(\"length of result_1 :\",len(results_1))\n",
    "        if  len(results_1)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_1 = results_1[-1]._json['id'] - 1\n",
    "        if  len(results_2)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_2 = results_2[-1]._json['id'] - 1\n",
    "        if  len(results_3)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_3 = results_3[-1]._json['id'] - 1\n",
    "        if  len(results_4)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_4 = results_4[-1]._json['id'] - 1\n",
    "        if  len(results_5)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_5 = results_5[-1]._json['id'] - 1\n",
    "        if  len(results_6)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_6 = results_6[-1]._json['id'] - 1\n",
    "        if  len(results_7)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_7 = results_7[-1]._json['id'] - 1\n",
    "        if  len(results_8)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_8 = results_8[-1]._json['id'] - 1\n",
    "        if  len(results_9)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_9 = results_9[-1]._json['id'] - 1\n",
    "        if  len(results_10)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_10 =results_10[-1]._json['id'] - 1\n",
    "        if  len(results_11)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_11 = results_11[-1]._json['id'] - 1\n",
    "        if  len(results_2)==0:\n",
    "            print(\"empty\")\n",
    "        else:\n",
    "            last_12 = results_12[-1]._json['id'] - 1\n",
    "        loop+=1\n",
    "        print(loop)\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_work(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()\n",
    "file4.close()\n",
    "file5.close()\n",
    "file5.close()\n",
    "file6.close()\n",
    "file7.close()\n",
    "file8.close()\n",
    "file9.close()\n",
    "file10.close()\n",
    "file11.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please note fot the puropse of submission we copied the New york times data collection in a new ipynb file,in the case of missing code please find the original scipt can be found in the textmining.ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
